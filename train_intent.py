import json
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
import pickle
import os

# ƒê∆∞·ªùng d·∫´n
DATA_PATH = "data/intents.json"
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# 1. ƒê·ªçc d·ªØ li·ªáu
with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

texts, labels = [], []
for intent in data["intents"]:
    for pattern in intent["patterns"]:
        texts.append(pattern)
        labels.append(intent["tag"])

# 2. M√£ h√≥a nh√£n
encoder = LabelEncoder()
y = encoder.fit_transform(labels)

# 3. Vector h√≥a vƒÉn b·∫£n
vectorizer = tf.keras.layers.TextVectorization(output_mode="tf_idf", max_tokens=1000)
text_ds = tf.data.Dataset.from_tensor_slices(texts).batch(32)
vectorizer.adapt(text_ds)
X = vectorizer(texts)

# 4. X√¢y d·ª±ng m√¥ h√¨nh
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X.shape[1],)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(len(set(labels)), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 5. Hu·∫•n luy·ªán
model.fit(X, y, epochs=200, verbose=0)
print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")

# 6. L∆∞u m√¥ h√¨nh v√† metadata
model.save(os.path.join(MODEL_DIR, "intent_model.keras"))

# L∆∞u vectorizer
vec_conf = {
    "vocab": vectorizer.get_vocabulary()
}
with open(os.path.join(MODEL_DIR, "vectorizer.json"), "w", encoding="utf-8") as f:
    json.dump(vec_conf, f, ensure_ascii=False, indent=2)

# L∆∞u encoder
with open(os.path.join(MODEL_DIR, "classes.json"), "w", encoding="utf-8") as f:
    json.dump(encoder.classes_.tolist(), f, ensure_ascii=False, indent=2)

with open(os.path.join(MODEL_DIR, "tag2id.pkl"), "wb") as f:
    pickle.dump(encoder, f)

print("üíæ ƒê√£ l∆∞u m√¥ h√¨nh v√† d·ªØ li·ªáu th√†nh c√¥ng!")
